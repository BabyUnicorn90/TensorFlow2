{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **영화 리뷰를 사용한 텍스트 분류**\n",
    "\n",
    "- 리뷰 텍스트를 긍정(positive), 부정(negative)로 분류\n",
    "- 이진(binary, =클래스(class))이 두개인 분류\n",
    "- 50,000개의 영화리뷰텍스트, 훈련용 테스트용 각각 25,000개 씩\n",
    "- 긍정리뷰수 = 부정리뷰수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "imdb = keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "#매개변수 num_words=10000: 훈련데이터에서 가장 많이 등장하는 상위 10,000개의 단어 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련샘플: 25000, 레이블: 25000\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "218 189\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련샘플: {}, 레이블: {}\".format(len(train_data), len(train_labels)))\n",
    "\n",
    "#첫번째 리뷰: \n",
    "print(train_data[0])  \n",
    "#리뷰 텍스트는 어휘사전의 특정 단어를 나타내는 정수로 변환되어 있음\n",
    "\n",
    "#첫번째 리뷰와 두번째 리뷰의 단어 개수 비교:\n",
    "print(len(train_data[0]), len(train_data[1]))\n",
    "#신경망의 입력은 길이가 같아야하기 때문에 나중에 이 문제 해결하도록!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정수를 단어로 다시 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK is an amazing actor and now the same being director <UNK father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK to the two little boy's that played the <UNK of norman and paul they were just brilliant children are often left out of the <UNK list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정수와 문자열을 매핑한 딕셔너리 객체에 질의하는 헬퍼함수 만들기:\n",
    "#단어와 정수 인덱스를 매핑한 딕셔너리 정의하기:\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "#처음 몇 개 인덱스는 사전에 정의되어 있음. \n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터준비\n",
    "\n",
    "리뷰(정수배열)은 신경망에 주입하기 전에 텐서로 변환되어야 함. \n",
    "\n",
    ">변환하는 방법:\n",
    "1. 원-핫 인코딩(one-hot encoding):\n",
    "    정수배열을 0과 1로 이루어진 벡터로 변환하고, \n",
    "    그 실수 벡터 데이터를 다룰 수 있는 층(dense 층)을 신경망의 첫번째 층으로 사용.\n",
    "    num_words * num_reviews 크기의 행렬이 필요하기 때문에 메모리 많이 사용.\n",
    "2. 정수배열의 길이가 모두 같도록 패딩(padding)을 추가, max_length * num_reviews 크기의 정수 텐서 만들기:\n",
    "    이런 형태의 텐서를 다룰 수 있는 임베딩(embadding)층을 첫번째 층으로 사용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "#두번째 방법을 사용하여 텐서로 변환하기: \n",
    "#영화 리뷰의 길이가 같아야 하므로 pad_sequence함수를 사용해 길이 맞추기:\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, \n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256\n",
    "                                                       )\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                      value=word_index[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                       maxlen=256\n",
    "                                                      )\n",
    "\n",
    "len(train_data[0]), len(train_data[1])\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성하기\n",
    "\n",
    "신경망은 층(layer)을 쌓아서 만든다. \n",
    "\n",
    "이 구조에서 결정해야 할 것: \n",
    "\n",
    "    1. 모델에서 얼마나 많은 층을 사용할 것인가?\n",
    "\n",
    "    2. 각 층에서 얼마나 많은 은닉유닛(hidden unit)을 사용할 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None, )))\n",
    "#첫번쨰 층: Embedding 층 \n",
    "#정수로 인코딩된 단어를 입력받고 각 단어 인덱스에 해당하는 임베딩 벡터 찾기,\n",
    "#그 벡터는 출력배열에 새로운 차원으로 추가\n",
    "#최종자원: (batch, sequence, embedding)\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "#두번째 층:\n",
    "#sequence차원에 대해 평균계산, 각 샘플에 대해 고정된 길이의 출력 벡터 반환.\n",
    "#길이가 다른 입력을 다루는 가장 간단한 방법! \n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "#세번째 층:\n",
    "#두번째 층까지 거쳐온 '고정길이의 출력벡터' --> 16개의 은닉유닛을 가진 완전연결층(3번째층. Dense층)\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "#네번째 층: \n",
    "#하나의 출력노드를 가진 완전연결층. \n",
    "#sigmoid활성화 함수를 이용, 0과1사이의 실수 출력  --> 확률/신뢰도\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _은닉유닛_\n",
    "출력의 개수는 층이 가진 표현공간(representational space) 차원.\n",
    "= 내부표현을 학습할 때 허용되는 네트워크 자유도의 양.\n",
    "\n",
    "### _손실함수와 옵티마이저_\n",
    "모델훈련에 필요한 적절한 손실함수와 옵티마이저 고르기: \n",
    "\n",
    "1. 손실함수: \n",
    "\n",
    "이 예제는 분류문제 & 확률출력 (<-- 출력층의 유닛: 1, & sigmoid 활성화 함수 사용) --> binary_crossentropy 손실함수 사용 \n",
    "\n",
    "- binary_crossentropy: 확률을 다룰 때 적절한 손실함수. 확률분포간 거리 측정. (타깃(정답) 분포와 예측 분포의 거리 측정)\n",
    "- mean squared error: 회귀문제(regression). 가격예측 등에 적절한 손실함수. \n",
    "\n",
    "2. 옵티마이저:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델이 사용할 옵티마이저와 손실함수 설정하기:\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증세트 만들기\n",
    "\n",
    "모델을 훈련할 때: 모델이 만난 적 없는 데이터에서 정확도 확인하도록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원본 훈련 데이터에서 10.000개의 샘플을 떼어네어 검증세트(validation set)로 만들기:\n",
    "#훈련데이터만을 사용하여 모델을 개발하고 튜닝하는 것이 목표.\n",
    "#테스트세트는 정확도 평가시 딱 한번만 사용.\n",
    "\n",
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val =  train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.6922 - accuracy: 0.5160 - val_loss: 0.6908 - val_accuracy: 0.6567\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.6874 - accuracy: 0.6946 - val_loss: 0.6837 - val_accuracy: 0.6925\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.6759 - accuracy: 0.7345 - val_loss: 0.6685 - val_accuracy: 0.7494\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.6543 - accuracy: 0.7649 - val_loss: 0.6428 - val_accuracy: 0.7626\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.6205 - accuracy: 0.7883 - val_loss: 0.6068 - val_accuracy: 0.7796\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.5763 - accuracy: 0.8096 - val_loss: 0.5637 - val_accuracy: 0.8006\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.5262 - accuracy: 0.8275 - val_loss: 0.5170 - val_accuracy: 0.8189\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.4753 - accuracy: 0.8448 - val_loss: 0.4721 - val_accuracy: 0.8335\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.4289 - accuracy: 0.8587 - val_loss: 0.4345 - val_accuracy: 0.8435\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.3888 - accuracy: 0.8737 - val_loss: 0.4029 - val_accuracy: 0.8530\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.3559 - accuracy: 0.8811 - val_loss: 0.3785 - val_accuracy: 0.8586\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.3279 - accuracy: 0.8892 - val_loss: 0.3588 - val_accuracy: 0.8638\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.3046 - accuracy: 0.8974 - val_loss: 0.3433 - val_accuracy: 0.8695\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.2852 - accuracy: 0.9019 - val_loss: 0.3304 - val_accuracy: 0.8742\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.2677 - accuracy: 0.9078 - val_loss: 0.3206 - val_accuracy: 0.8754\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.2526 - accuracy: 0.9130 - val_loss: 0.3127 - val_accuracy: 0.8783\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.2390 - accuracy: 0.9175 - val_loss: 0.3064 - val_accuracy: 0.8778\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.2266 - accuracy: 0.9214 - val_loss: 0.3003 - val_accuracy: 0.8804\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.2153 - accuracy: 0.9251 - val_loss: 0.2965 - val_accuracy: 0.8819\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.2049 - accuracy: 0.9295 - val_loss: 0.2926 - val_accuracy: 0.8833\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1956 - accuracy: 0.9324 - val_loss: 0.2899 - val_accuracy: 0.8836\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1865 - accuracy: 0.9379 - val_loss: 0.2880 - val_accuracy: 0.8845\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1782 - accuracy: 0.9410 - val_loss: 0.2866 - val_accuracy: 0.8835\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1704 - accuracy: 0.9447 - val_loss: 0.2868 - val_accuracy: 0.8850\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1632 - accuracy: 0.9477 - val_loss: 0.2857 - val_accuracy: 0.8847\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1565 - accuracy: 0.9511 - val_loss: 0.2852 - val_accuracy: 0.8857\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1496 - accuracy: 0.9541 - val_loss: 0.2857 - val_accuracy: 0.8851\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1435 - accuracy: 0.9559 - val_loss: 0.2865 - val_accuracy: 0.8852\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1379 - accuracy: 0.9590 - val_loss: 0.2876 - val_accuracy: 0.8851\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1324 - accuracy: 0.9605 - val_loss: 0.2889 - val_accuracy: 0.8860\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.1269 - accuracy: 0.9634 - val_loss: 0.2907 - val_accuracy: 0.8860\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1220 - accuracy: 0.9651 - val_loss: 0.2934 - val_accuracy: 0.8855\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1171 - accuracy: 0.9663 - val_loss: 0.2950 - val_accuracy: 0.8846\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1130 - accuracy: 0.9681 - val_loss: 0.2979 - val_accuracy: 0.8855\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1080 - accuracy: 0.9706 - val_loss: 0.3014 - val_accuracy: 0.8837\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1044 - accuracy: 0.9709 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0997 - accuracy: 0.9735 - val_loss: 0.3060 - val_accuracy: 0.8832\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0958 - accuracy: 0.9744 - val_loss: 0.3091 - val_accuracy: 0.8828\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0923 - accuracy: 0.9760 - val_loss: 0.3119 - val_accuracy: 0.8834\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0886 - accuracy: 0.9771 - val_loss: 0.3161 - val_accuracy: 0.8817\n"
     ]
    }
   ],
   "source": [
    "#이 모델을 512개의 샘플로 이루어진 미니배치(mini-batch)에서 \n",
    "#40번의 epoch동안 훈련.\n",
    "#=x_train과 y_train텐서에 있는 모든 샘플에 대해 40번 반복.\n",
    "#훈련하는동안 10,000개의 검증세트에서 모델의 손실과 정확도 모니터링.\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 1s - loss: 0.3359 - accuracy: 0.8711\n",
      "[0.33594506986618045, 0.87112]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도와 손실그래프 그리기 \n",
    "\n",
    "model.fit()d은 History 객체 반환. \n",
    "훈련하는동안 일어난 모든 정보가 담긴 딕셔너리가 들어있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
